{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8246b47c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6158b-fd77-4c58-a702-a6ba72b9314b",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "Run the following cell to import libraries and helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0ecb3e-92f3-481a-9981-14c47866a4b5",
   "metadata": {
    "deletable": true,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "        Layer (type)              Output Shape         Param #\n",
      "======================================================================\n",
      "            Linear-1                 [64, 516]          66,564\n",
      "              ReLU-2                 [64, 516]               0\n",
      "            Linear-3                 [64, 516]         266,772\n",
      "              ReLU-4                 [64, 516]               0\n",
      "            Linear-5                   [64, 1]             517\n",
      "         SimpleMLP-6                   [64, 1]               0\n",
      "======================================================================\n",
      "Total params: 333,853\n",
      "Trainable params: 333,853\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=516, output_size=1):\n",
    "        super().__init__()\n",
    "        self.fc1   = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3   = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Move model to GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SimpleMLP().to(device)\n",
    "\n",
    "from custom_torchinfo import custom_summary\n",
    "custom_summary(model, input_size=(64, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Checkpoint 1/3\n",
    "\n",
    "Create the `model_size_bytes` function that returns a model's size in bytes (parameters + buffers). The function should have a single input for a PyTorch model.\n",
    "\n",
    "Use the function to calculate the byte size of the model created using the `SimpleMLP` class in the previous cell. Be sure to run the setup cell above to instantiate the model in the variable `model`. \n",
    "\n",
    "Print out the model size converted into megabytes (MB) by dividing the number of bytes by `1024**2`.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model Size] 1,335,412 bytes ~ 1.274 MB\n"
     ]
    }
   ],
   "source": [
    "def model_size_bytes(model):\n",
    "    \"\"\"Return model size in bytes (params + buffers).\"\"\"\n",
    "    model_params = list(model.parameters()) + list(model.buffers())\n",
    "    size = 0\n",
    "    for t in model_params:\n",
    "        size += t.numel() * t.element_size()\n",
    "    return size\n",
    "    \n",
    "size_bytes = model_size_bytes(model)\n",
    "\n",
    "# Show output\n",
    "print(f\"[Model Size] {size_bytes:,} bytes ~ {size_bytes / (1024**2):.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Checkpoint 2/3\n",
    "\n",
    "Create the `measure_latency` function that returns a model's latency in milliseconds (ms) when passing input data through its forward pass. The function should have three inputs:\n",
    "- `model`: PyTorch model to test.\n",
    "- `x`: Input data.\n",
    "- `iters`: Number of iterations to calculate the average latency (ms). \n",
    "\n",
    "Use the function to calculate the latency of the model from before on synthetic input data with different batch sizes:\n",
    "- The first batch should have a batch size of `64` with an input size of `128` dimensions. Apply the function to the first batch to calculate the latency over `50` iterations and save the average latency to the variable `latency_x1`.\n",
    "- The second batch should have a smaller batch size of `8` with an input size of `128` dimensions. Apply the function to the second batch to calculate the latency over `50` iterations and save the average latency to the variable `latency_x2`.\n",
    "\n",
    "Print and compare the latencies of the model processing data with different batch sizes. Which do you expect to be faster?\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Latency] 0.136 ms per forward pass (batch size=64)\n",
      "[Latency] 0.122 ms per forward pass (batch size=8)\n"
     ]
    }
   ],
   "source": [
    "# Create test batches -- DO NOT MODIFY\n",
    "x_base = torch.randn(1, 128, device=device)\n",
    "batch_size1 = 64\n",
    "x1 = x_base.expand(batch_size1, -1).contiguous()\n",
    "batch_size2 = 8\n",
    "x2 = x_base.expand(batch_size2, -1).contiguous()\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "def measure_latency(model, x, iters=50):\n",
    "    \"\"\"Return average latency (ms) per forward pass.\"\"\"\n",
    "    model.eval()\n",
    "    start = time.perf_counter()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(iters):\n",
    "            _ = model(x)\n",
    "            if x.device.type == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    latency = elapsed/iters * 1e3\n",
    "    return latency\n",
    "\n",
    "latency_x1 = measure_latency(model, x1, iters=50)\n",
    "latency_x2 = measure_latency(model, x2, iters=50)\n",
    "\n",
    "# Show output\n",
    "print(f\"[Latency] {latency_x1:.3f} ms per forward pass (batch size={batch_size1})\")\n",
    "print(f\"[Latency] {latency_x2:.3f} ms per forward pass (batch size={batch_size2})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfdae87-f84c-4758-b163-b427f7b65d10",
   "metadata": {},
   "source": [
    "#### Checkpoint 3/3\n",
    "\n",
    "Create the `measure_gpu_memory` function that returns a model's GPU memory currently being used **and** the peak allocated memory used when the model passes input data through its forward pass. The function should have two inputs:\n",
    "- `model`: PyTorch model to test.\n",
    "- `x`: Input data.\n",
    "\n",
    "The function should return two outputs:\n",
    "- `current`: The GPU memory currently being allocated by the model.\n",
    "- `peak` The peak allocated memory used when the model passes input data through its forward pass.\n",
    "\n",
    "Use the function to calculate the difference in memory allocations using the same input as before, but with different batch sizes. Save the memory calculation of the first batch to the variable `mem1` and the second batch to the variable `mem2`. \n",
    "\n",
    "Print and compare the memory allocations.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ddf801b",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp3"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU Memory] Current allocated: 9.44 MB | Peak during forward: 10.69 MB (batch size=64)\n",
      "[GPU Memory] Current allocated: 9.44 MB | Peak during forward: 10.47 MB (batch size=8)\n"
     ]
    }
   ],
   "source": [
    "# Create test batches -- DO NOT MODIFY\n",
    "x_base = torch.randn(1, 128, device=device)\n",
    "batch_size1 = 64\n",
    "x1 = x_base.expand(batch_size1, -1).contiguous()\n",
    "batch_size2 = 8\n",
    "x2 = x_base.expand(batch_size2, -1).contiguous()\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "def measure_gpu_memory(model, x):\n",
    "    \"\"\"Return current and peak GPU memory in MB after one forward.\"\"\"\n",
    "    torch.cuda.empty_cache()               \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y = model(x)\n",
    "        \n",
    "    torch.cuda.synchronize()\n",
    "    current = torch.cuda.memory_allocated() / (1024**2)\n",
    "    peak    = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    return current, peak\n",
    "\n",
    "mem1 = measure_gpu_memory(model, x1)\n",
    "mem2 = measure_gpu_memory(model, x2)\n",
    "\n",
    "# Show output\n",
    "print(f\"[GPU Memory] Current allocated: {mem1[0]:.2f} MB | Peak during forward: {mem1[1]:.2f} MB (batch size={batch_size1})\")\n",
    "print(f\"[GPU Memory] Current allocated: {mem2[0]:.2f} MB | Peak during forward: {mem2[1]:.2f} MB (batch size={batch_size2})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f187a-ce8a-44ca-b80d-b1986ced41d7",
   "metadata": {},
   "source": [
    "#### Clean up session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad0eb6-2a92-445d-9b7a-86a9fb67e4f1",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "clean"
    ]
   },
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "del model, x1, x2 \n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d21cad-8bba-4127-a1fd-be5b0848a85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
